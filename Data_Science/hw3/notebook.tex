
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{hw3}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    Before you turn this problem in, make sure everything runs as expected.
First, \textbf{restart the kernel} (in the menubar, select
Kernel\(\rightarrow\)Restart) and then \textbf{run all cells} (in the
menubar, select Cell\(\rightarrow\)Run All).

Make sure you fill in any place that says \texttt{YOUR\ CODE\ HERE} or
"YOUR ANSWER HERE", as well as your name and collaborators below:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{n}{NAME} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Weijie Yuan}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{COLLABORATORS} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{N/A}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \section{Homework 3: Loss
Minimization}\label{homework-3-loss-minimization}

\subsection{Modeling, Estimation and Gradient
Descent}\label{modeling-estimation-and-gradient-descent}

\subsection{Due Date: Tuesday 10/9, 11:59
PM}\label{due-date-tuesday-109-1159-pm}

\subsection{Course Policies}\label{course-policies}

Here are some important course policies. These are also located at
http://www.ds100.org/fa18/.

\textbf{Collaboration Policy}

Data science is a collaborative activity. While you may talk with others
about the homework, we ask that you \textbf{write your solutions
individually}. If you do discuss the assignments with others please
\textbf{include their names} at the top of your solution.

\subsection{This Assignment}\label{this-assignment}

In this homework, we explore modeling data, estimating optimal
parameters and a numerical estimation method, gradient descent. These
concepts are some of the fundamentals of data science and machine
learning and will serve as the building blocks for future projects,
classes, and work.

After this homework, you should feel comfortable with the following:

\begin{itemize}
\item
  Practice reasoning about a model
\item
  Build some intuition for loss functions and how they behave
\item
  Work through deriving the gradient of a loss with respect to model
  parameters
\item
  Work through a basic version of gradient descent.
\end{itemize}

This homework is comprised of completing code, deriving analytic
solutions, writing LaTex and visualizing loss.

\subsection{Submission - IMPORTANT, PLEASE
READ}\label{submission---important-please-read}

For this assignment and future assignments (homework and projects) you
will also submit your free response and plotting questions to
gradescope. To do this, you can download as PDF
(\texttt{File-\textgreater{}Download\ As-\textgreater{}PDF\ via\ Latex\ (.pdf)}).
You are responsible for submitting and tagging your answers in
gradescope. For each free response and plotting question, please
include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Relevant code used to generate the plot or inform your insights
\item
  The written free response or plot
\end{enumerate}

We are doing this to make it easier on our graders and for you, in the
case you need to submit a regrade request. Gradescope (as of now) is
still better for manual grading.

\subsection{Score breakdown}\label{score-breakdown}

\begin{longtable}[]{@{}ll@{}}
\toprule
Question & Points\tabularnewline
\midrule
\endhead
Question 1a & 1\tabularnewline
Question 1b & 1\tabularnewline
Question 1c & 1\tabularnewline
Question 1d & 1\tabularnewline
Question 1e & 1\tabularnewline
Question 2a & 2\tabularnewline
Question 2b & 1\tabularnewline
Question 2c & 1\tabularnewline
Question 2d & 1\tabularnewline
Question 2e & 1\tabularnewline
Question 2f & 1\tabularnewline
Question 3a & 1\tabularnewline
Question 3b & 3\tabularnewline
Question 3c & 2\tabularnewline
Question 4a & 3\tabularnewline
Question 4b & 1\tabularnewline
Question 4c & 1\tabularnewline
Question 4d & 1\tabularnewline
Question 4e & 1\tabularnewline
Question 5a & 2\tabularnewline
Question 5b & 4\tabularnewline
Question 5c & 0\tabularnewline
Question 5d & 0\tabularnewline
Question 6a & 3\tabularnewline
Question 6b & 3\tabularnewline
Question 6c & 3\tabularnewline
Question 6d & 3\tabularnewline
Question 6e & 3\tabularnewline
Question 6f & 3\tabularnewline
Question 6g & 3\tabularnewline
Question 7a & 1\tabularnewline
Question 7b & 1\tabularnewline
Question 7c & 1\tabularnewline
Question 7d & 1\tabularnewline
Question 7e & 0\tabularnewline
Total & 56\tabularnewline
\bottomrule
\end{longtable}

    \section{Getting Started}\label{getting-started}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Imports}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{csv}
        \PY{k+kn}{import} \PY{n+nn}{re}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        
        \PY{c+c1}{\PYZsh{} Set some parameters}
        \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{font.size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{16}
        \PY{n}{np}\PY{o}{.}\PY{n}{set\PYZus{}printoptions}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} We will use plot\PYZus{}3d helper function to help us visualize gradient}
        \PY{k+kn}{from} \PY{n+nn}{hw3\PYZus{}utils} \PY{k}{import} \PY{n}{plot\PYZus{}3d}
\end{Verbatim}


    \subsection{Load Data}\label{load-data}

Load the data.csv file into a pandas dataframe.\\
Note that we are reading the data directly from the URL address.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} Run this cell to load our sample data}
        \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{https://github.com/DS\PYZhy{}100/fa18/raw/gh\PYZhy{}pages/assets/datasets/hw3\PYZus{}data.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:}           x         y
        0 -5.000000 -7.672309
        1 -4.966555 -7.779735
        2 -4.933110 -7.995938
        3 -4.899666 -8.197059
        4 -4.866221 -8.183883
\end{Verbatim}
            
    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{1: A Simple Model}\label{a-simple-model}

Let's start by examining our data and creating a simple model that can
represent this data.

    \subsubsection{Question 1}\label{question-1}

\paragraph{Question 1a}\label{question-1a}

First, let's visualize the data in a scatter plot. After implementing
the \texttt{scatter} function below, you should see something like this:
\includegraphics{scatter.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Generate a scatter plot using x and y}
        
        \PY{l+s+sd}{    Keyword arguments:}
        \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
        \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{s}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
        
        \PY{n}{x} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{y} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Question 1b}\label{question-1b}

Describe any significant observations about the distribution of the
data. How can you describe the relationship between \(x\) and \(y\)?

    y has posive correlation with x and the slope of this positive
correlation shows nearly periodic change. It looks roughly linear, with
some extra noise terms.

    \paragraph{Question 1c}\label{question-1c}

The data looks roughly linear, with some extra noise. For now, let's
assume that the data follows some underlying linear model. We define the
underlying linear model that predicts the value \(y\) using the value
\(x\) as: \(f_{\theta^*}(x) = \theta^* \cdot x\)

Since we cannot find the value of the population parameter \(\theta^*\)
exactly, we will assume that our dataset approximates our population and
use our dataset to estimate \(\theta^*\). We denote our estimation with
\(\theta\), our fitted estimation with \(\hat{\theta}\), and our model
as:

\[\Large
f_{\theta}(x) = \theta \cdot x
\]

Based on this equation, define the linear model function
\texttt{linear\_model} below to estimate \(\textbf{y}\) (the
\(y\)-values) given \(\textbf{x}\) (the \(x\)-values) and \(\theta\).
This model is similar to the model you defined in Lab 5: Modeling and
Estimation.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{linear\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Returns the estimate of y given x and theta}
        
        \PY{l+s+sd}{    Keyword arguments:}
        \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
        \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the scalar theta}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{y} \PY{o}{=} \PY{n}{x}\PY{o}{*}\PY{n}{theta}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
            \PY{k}{return} \PY{n}{y}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k}{assert} \PY{n}{linear\PYZus{}model}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}
        \PY{k}{assert} \PY{n}{linear\PYZus{}model}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{100}
        \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{linear\PYZus{}model}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{24}
        \PY{k}{assert} \PY{n}{linear\PYZus{}model}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{30}
\end{Verbatim}


    \paragraph{Question 1d}\label{question-1d}

In class, we learned that the \(L^2\) (or squared) loss function is
smooth and continuous. Let's use \(L^2\) loss to evaluate our estimate
\(\theta\), which we will use later to identify an optimal \(\theta\),
represented as \(\hat{\theta}\). Define the \(L^2\) loss function
\texttt{l2\_loss} below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{def} \PY{n+nf}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}hat}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Returns the average l2 loss given y and y\PYZus{}hat}
        
        \PY{l+s+sd}{    Keyword arguments:}
        \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of true values y}
        \PY{l+s+sd}{    y\PYZus{}hat \PYZhy{}\PYZhy{} the vector of predicted values y\PYZus{}hat}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{y\PYZus{}hat}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}
        \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{4} 
        \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{16}
        \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mf}{20.5}
        \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mf}{6.0}
\end{Verbatim}


    \paragraph{Question 1e}\label{question-1e}

First, visualize the \(L^2\) loss as a function of \(\theta\), where
several different values of \(\theta\) are given. Be sure to label your
axes properly. You plot should look something like this:
\includegraphics{l2_avg_loss.png}

What looks like the optimal value, \(\hat{\theta}\), based on the
visualization? Set \texttt{theta\_star\_guess} to the value of
\(\theta\) that appears to minimize our loss.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{visualize}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{thetas}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Plots the average l2 loss for given x, y as a function of theta.}
         \PY{l+s+sd}{    Use the functions you wrote for linear\PYZus{}model and l2\PYZus{}loss.}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    thetas \PYZhy{}\PYZhy{} an array containing different estimates of the scalar theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{avg\PYZus{}loss} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{p}{[}\PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{x}\PY{o}{*}\PY{n}{theta}\PY{p}{)} \PY{k}{for} \PY{n}{theta} \PY{o+ow}{in} \PY{n}{thetas}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} Calculate the loss here for each value of theta}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{thetas}\PY{p}{,}\PY{n}{avg\PYZus{}loss}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Theta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Average Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             
         \PY{n}{thetas} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{70}\PY{p}{)}
         \PY{n}{visualize}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{thetas}\PY{p}{)}
         \PY{n}{theta\PYZus{}star\PYZus{}guess} \PY{o}{=} \PY{l+m+mf}{1.5}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}
         \PY{k}{assert} \PY{n}{l2\PYZus{}loss}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{100}
         \PY{k}{assert} \PY{l+m+mi}{1} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{theta\PYZus{}star\PYZus{}guess} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{2}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{2: Fitting our Simple Model}\label{fitting-our-simple-model}

Now that we have defined a simple linear model and loss function, let's
begin working on fitting our model to the data.

\subsubsection{Question 2}\label{question-2}

Let's confirm our visual findings for optimal \(\hat{\theta}\).

\paragraph{Question 2a}\label{question-2a}

First, find the analytical solution for the optimal \(\hat{\theta}\) for
average \(L^2\) loss. Write up your solution in the cell below using
LaTex.

Hint: notice that we now have \(\textbf{x}\) and \(\textbf{y}\) instead
of \(x\) and \(y\). This means that when writing the loss function
\(L(\textbf{x}, \textbf{y}, \theta)\), you'll need to take the average
of the squared losses for each \(y_i\), \(f_\theta(x_i)\) pair. For tips
on getting started, see chapter
\href{chapter\%2010}{https://www.textbook.ds100.org/ch/10/modeling\_loss\_functions.html}
of the textbook. Note that if you click "Open in DataHub", you can
access the LaTeX source code of the book chapter, which you might find
handy for typing up your work. Show your work, i.e. don't just write the
answer.

    \[
\begin{aligned}
L(\textbf{x}, \textbf{y}, \theta)
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - f_{\theta}(x_i))^2\\
\frac{\partial}{\partial \theta} L(\textbf{x}, \textbf{y}, \theta)
&= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - x_i\theta)*x_i \\
&= -\frac{2}{n} \sum_{i = 1}^{n} (y_i - x_i\theta)*x_i
\end{aligned}
\]

If we want to find the analytical solution for the optimal \(\theta\),
we can set the derivative of loss function with respect to \(\theta\) to
zero.

\[
\begin{aligned}
-\frac{2}{n} \sum_{i = 1}^{n} (y_i - x_i\theta)*x_i &= 0 \\
\sum_{i = 1}^{n} (y_i - x_i\theta)*x_i &= 0 \\
\sum_{i = 1}^{n} y_i*x_i - \sum_{i = 1}^{n} \theta x_i^2 &= 0 \\
\theta \sum_{i = 1}^{n} x_i^2 &= \sum_{i = 1}^{n} y_i x_i \\
\theta &= \frac{\sum_{i = 1}^{n} y_i x_i}{\sum_{i = 1}^{n} x_i^2} \\
\hat \theta = \theta &= \frac{\textbf{y}^T \textbf{x}}{\textbf{x}^T \textbf{x}}
\end{aligned}
\]

Validation: \[
\begin{aligned}
\frac{\partial^2}{\partial \theta^2} L(\textbf{x}, \textbf{y}, \theta)&=
\frac{2}{n} \sum_{i = 1}^{n} x^2_i\geq0
\end{aligned}
\]

So,
\(\hat \theta = \theta = \frac{\textbf{y}^T \textbf{x}}{\textbf{x}^T \textbf{x}}\)
minimizes the loss function.

    \paragraph{Question 2b}\label{question-2b}

Now that we have the analytic solution for \(\hat{\theta}\), implement
the function \texttt{find\_theta} that calculates the numerical value of
\(\hat{\theta}\) based on our data \(\textbf{x}\), \(\textbf{y}\).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{def} \PY{n+nf}{find\PYZus{}theta}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Find optimal theta given x and y}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{theta\PYZus{}opt} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{y}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{o}{/}\PY{n+nb}{sum}\PY{p}{(}\PY{n}{x}\PY{o}{*}\PY{n}{x}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{k}{return} \PY{n}{theta\PYZus{}opt}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{t\PYZus{}hat} \PY{o}{=} \PY{n}{find\PYZus{}theta}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{theta\PYZus{}opt = }\PY{l+s+si}{\PYZob{}t\PYZus{}hat\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{k}{assert} \PY{l+m+mf}{1.4} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{t\PYZus{}hat} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mf}{1.6}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
theta\_opt = 1.5502648085962216

    \end{Verbatim}

    \paragraph{Question 2c}\label{question-2c}

Now, let's plot our loss function again using the \texttt{visualize}
function. But this time, add a vertical line at the optimal value of
theta (plot the line \(x = \hat{\theta}\)). Your plot should look
something like this: \includegraphics{vertical_linear.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{theta\PYZus{}opt} \PY{o}{=} \PY{n}{t\PYZus{}hat}
         \PY{n}{visualize}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{thetas}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{axvline}\PY{p}{(}\PY{n}{t\PYZus{}hat}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Question 2d}\label{question-2d}

We now have an optimal value for \(\theta\) that minimizes our loss. In
the cell below, plot the scatter plot of the data from Question 1a (you
can reuse the \texttt{scatter} function here). But this time, add the
line \(f_{\hat{\theta}}(x) = \hat{\theta} \cdot \textbf{x}\) using the
\(\hat{\theta}\) you computed above. Your plot should look something
like this: \includegraphics{scatter_with_line.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{theta\PYZus{}opt} \PY{o}{=} \PY{n}{t\PYZus{}hat}
         \PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n}{x\PYZus{}line} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{x}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}line}\PY{p}{,}\PY{n}{x\PYZus{}line}\PY{o}{*}\PY{n}{t\PYZus{}hat}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Question 2e}\label{question-2e}

Great! It looks like our estimator \(f_{\hat{\theta}}(x)\) is able to
capture a lot of the data with a single parameter \(\theta\). Now let's
try to remove the linear portion of our model from the data to see if we
missed anything.

The remaining data is known as the residual,
\(\textbf{r}=\textbf{y}-\hat{\theta} \cdot \textbf{x}\). Below, write a
function to find the residual and plot the residuals corresponding to
\(x\) in a scatter plot. Plot a horizontal line at \(y=0\) to assist
visualization.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k}{def} \PY{n+nf}{visualize\PYZus{}residual}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Plot a scatter plot of the residuals, the remaining }
         \PY{l+s+sd}{    values after removing the linear model from our data.}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{n}{r} \PY{o}{=} \PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{t\PYZus{}hat}\PY{o}{*}\PY{n}{x}
             \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{r}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{visualize\PYZus{}residual}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Question 2f}\label{question-2f}

What does the residual look like? Do you notice a relationship between
\(x\) and \(r\)?

    The residual looks like a sin/cos function graphic fluctuating up and
down around y=0. Residual is kind of periodic with respect to x with
some extra noise terms.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{3: Increasing Model
Complexity}\label{increasing-model-complexity}

It looks like the remaining data is sinusoidal, meaning our original
data follows a linear function and a sinusoidal function. Let's define a
new model to address this discovery and find optimal parameters to best
fit the data:

\[\Large
f_\boldsymbol\theta(x) = \theta_1x + sin(\theta_2x)
\]

Now, our model is parameterized by both \(\theta_1\) and \(\theta_2\),
or composed together, \(\boldsymbol{\theta}\).

Note that a generalized sine function \(a\sin(bx+c)\) has three
parameters: amplitude scaling parameter \(a\), frequency parameter \(b\)
and phase shifting parameter \(c\). Looking at the residual plot above,
it looks like the residual is zero at x = 0, and the residual swings
between -1 and 1. Thus, it seems reasonable to effectively set the
scaling and phase shifting parameter (\(a\) and \(c\) in this case) to 1
and 0 respectively. While we could try to fit \(a\) and \(c\), we're
unlikely to get much benefit. When you're done with the homework, you
can try adding \(a\) and \(c\) to our model and fitting these values to
see if you can get a better loss.

    \paragraph{Question 3a}\label{question-3a}

As in Question 1, fill in the \texttt{sin\_model} function that predicts
\(\textbf{y}\) (the \(y\)-values) using \(\textbf{x}\) (the
\(x\)-values), but this time based on our new equation.

\emph{Hint:} Try to do this without using for loops. The \texttt{np.sin}
function may help you.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{def} \PY{n+nf}{sin\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{theta\PYZus{}1}\PY{p}{,} \PY{n}{theta\PYZus{}2}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Predict the estimate of y given x, theta\PYZus{}1, theta\PYZus{}2}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    theta\PYZus{}1 \PYZhy{}\PYZhy{} the scalar value theta\PYZus{}1}
         \PY{l+s+sd}{    theta\PYZus{}2 \PYZhy{}\PYZhy{} the scalar value theta\PYZus{}2}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{y} \PY{o}{=} \PY{n}{theta\PYZus{}1}\PY{o}{*}\PY{n}{x}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta\PYZus{}2}\PY{o}{*}\PY{n}{x}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{k}{return} \PY{n}{y}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{sin\PYZus{}model}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{1.0000000000000002}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Check that we accept x as arrays}
         \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{sin\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1}
\end{Verbatim}


    \paragraph{Question 3b}\label{question-3b}

Use the average \(L^2\) loss to compute
\(\frac{\partial L }{\partial \theta_1}, \frac{\partial L }{\partial \theta_2}\).

First, we will use LaTex to write
\(L(\textbf{x}, \textbf{y}, \theta_1, \theta_2)\),
\(\frac{\partial L }{\partial \theta_1}\), and
\(\frac{\partial L }{\partial \theta_2}\) given \(\textbf{x}\),
\(\textbf{y}\), \(\boldsymbol{\theta}\).

You don't need to write out the full derivation. Just the final
expression is fine.

    \[
\begin{aligned}
L(\textbf{x}, \textbf{y}, \boldsymbol{\theta})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - f_{\theta}(x_i))^2\\
L(\textbf{x}, \textbf{y}, \boldsymbol{\theta})
&= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta_1 x_i-sin(\theta_2 x_i))^2\\
\frac{\partial}{\partial \theta_1} L(\textbf{x}, \textbf{y}, \boldsymbol{\theta})
&= \frac{-2}{n} \sum_{i = 1}^{n}[(y_i - \theta_1 x_i-sin(\theta_2 x_i)) x_i] \\
\frac{\partial}{\partial \theta_2} L(\textbf{x}, \textbf{y}, \boldsymbol{\theta})
&= \frac{-2}{n} \sum_{i = 1}^{n}[(y_i - \theta_1 x_i-sin(\theta_2 x_i))cos(\theta_2 x_i) x_i] \\
\end{aligned}
\]

    \paragraph{Question 3c}\label{question-3c}

Now, implement the functions \texttt{dt1} and \texttt{dt2}, which should
compute \(\frac{\partial L }{\partial \theta_1}\) and
\(\frac{\partial L }{\partial \theta_2}\) respectively. Use the formulas
you wrote for \(\frac{\partial L }{\partial \theta_1}\) and
\(\frac{\partial L }{\partial \theta_2}\) in the previous exercise. In
the functions below, the parameter \texttt{theta} is a vector that looks
like \(( \theta_1, \theta_2 )\).

Note: To keep your code a bit more concise, be aware that
\texttt{np.mean} does the same thing as \texttt{np.sum} divided by the
length of the numpy array.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k}{def} \PY{n+nf}{dt1}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute the numerical value of the partial of l2 loss with respect to theta\PYZus{}1}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of all x values}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of all y values}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{n}{x}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k}{def} \PY{n+nf}{dt2}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute the numerical value of the partial of l2 loss with respect to theta\PYZus{}2}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of all x values}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of all y values}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{o}{*}\PY{n}{x}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{} This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.}
         \PY{k}{def} \PY{n+nf}{dt}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Returns the gradient of l2 loss with respect to vector theta}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{dt1}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{theta}\PY{p}{)}\PY{p}{,} \PY{n}{dt2}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{theta}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{dt1}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{25.376660670924529}\PY{p}{)}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{dt2}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{1.9427210155296564}\PY{p}{)}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{4: Gradient Descent}\label{gradient-descent}

Now try to solve for the optimal \(\hat{\theta}\) analytically...

\textbf{Just kidding!}

You can try but we don't recommend it. When finding an analytic solution
becomes difficult or impossible, we resort to alternative optimization
methods for finding an approximate solution.

\subsubsection{Question 4}\label{question-4}

So let's try implementing a numerical optimization method: gradient
descent!

\paragraph{Question 4a}\label{question-4a}

Implement the \texttt{grad\_desc} function that performs gradient
descent for a finite number of iterations. This function takes in an
array for \(\textbf{x}\) (\texttt{x}), an array for \(\textbf{y}\)
(\texttt{y}), and an initial value for \(\theta\) (\texttt{theta}).
\texttt{alpha} will be the learning rate (or step size, whichever term
you prefer). In this part, we'll use a static learning rate that is the
same at every time step.

At each time step, use the gradient and \texttt{alpha} to update your
current \texttt{theta}. Also at each time step, be sure to save the
current \texttt{theta} in \texttt{theta\_history}, along with the
\(L^2\) loss (computed with the current \texttt{theta}) in
\texttt{loss\_history}.

Hints: - Write out the gradient update equation (1 step). What variables
will you need for each gradient update? Of these variables, which ones
do you already have, and which ones will you need to recompute at each
time step? - You may need a loop here to update \texttt{theta} several
times - Recall that the gradient descent update function follows the
form: \[\large
\boldsymbol\theta^{(t+1)} \leftarrow \boldsymbol\theta^{(t)} - \alpha \left(\nabla_\boldsymbol\theta \mathbf{L}(\textbf{x}, \textbf{y}, \boldsymbol\theta^{(t)}) \right)
\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{} Run me}
         \PY{k}{def} \PY{n+nf}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{k}{def} \PY{n+nf}{grad\PYZus{}desc}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Run gradient descent update for a finite number of iterations and static learning rate}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta to use at first iteration}
         \PY{l+s+sd}{    num\PYZus{}iter \PYZhy{}\PYZhy{} the max number of iterations}
         \PY{l+s+sd}{    alpha \PYZhy{}\PYZhy{} the learning rate (also called the step size)}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Return:}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the optimal value of theta after num\PYZus{}iter of gradient descent}
         \PY{l+s+sd}{    theta\PYZus{}history \PYZhy{}\PYZhy{} the series of theta values over each iteration of gradient descent}
         \PY{l+s+sd}{    loss\PYZus{}history \PYZhy{}\PYZhy{} the series of loss values over each iteration of gradient descent}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{theta\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{loss\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}iter}\PY{p}{)}\PY{p}{:}
                 \PY{n}{theta} \PY{o}{=} \PY{n}{theta} \PY{o}{\PYZhy{}} \PY{n}{alpha} \PY{o}{*} \PY{n}{dt}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}
                 \PY{n}{theta\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{theta}\PY{p}{)}
                 \PY{n}{loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                 \PY{n}{loss\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{k}{return} \PY{n}{theta}\PY{p}{,} \PY{n}{theta\PYZus{}history}\PY{p}{,} \PY{n}{loss\PYZus{}history}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{t} \PY{o}{=} \PY{n}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}
         \PY{n}{t\PYZus{}est}\PY{p}{,} \PY{n}{ts}\PY{p}{,} \PY{n}{loss} \PY{o}{=} \PY{n}{grad\PYZus{}desc}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{t}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}
         
         \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{ts}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{loss}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{20} \PY{c+c1}{\PYZsh{} theta history and loss history are 20 items in them}
         \PY{k}{assert} \PY{n}{ts}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{)} \PY{c+c1}{\PYZsh{} theta history contains theta values}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isscalar}\PY{p}{(}\PY{n}{loss}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} loss history is a list of scalar values, not vector}
         
         \PY{k}{assert} \PY{n}{loss}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{loss}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} loss is decreasing}
         
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{t\PYZus{}est}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{4.5}\PY{p}{,} \PY{n}{atol}\PY{o}{=}\PY{l+m+mf}{2e\PYZhy{}1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} theta\PYZus{}est should be close to our value}
\end{Verbatim}


    \paragraph{Question 4b}\label{question-4b}

Now, let's try using a decaying learning rate. Implement
\texttt{grad\_desc\_decay} below, which performs gradient descent with a
learning rate that decreases slightly with each time step. You should be
able to copy most of your work from the previous part, but you'll need
to tweak how you update \texttt{theta} at each time step.

By decaying learning rate, we mean instead of just a number \(\alpha\),
the learning should be now \(\frac{\alpha}{i+1}\) where \(i\) is the
current number of iteration. (Why do we need to add '+ 1' in the
denominator?)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k}{def} \PY{n+nf}{grad\PYZus{}desc\PYZus{}decay}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Run gradient descent update for a finite number of iterations and decaying learning rate}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    num\PYZus{}iter \PYZhy{}\PYZhy{} the max number of iterations}
         \PY{l+s+sd}{    alpha \PYZhy{}\PYZhy{} the learning rate}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Return:}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the optimal value of theta after num\PYZus{}iter of gradient descent}
         \PY{l+s+sd}{    theta\PYZus{}history \PYZhy{}\PYZhy{} the series of theta values over each iteration of gradient descent}
         \PY{l+s+sd}{    loss\PYZus{}history \PYZhy{}\PYZhy{} the series of loss values over each iteration of gradient descent}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{theta\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{loss\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}iter}\PY{p}{)}\PY{p}{:}
                 \PY{n}{alpha\PYZus{}update} \PY{o}{=} \PY{n}{alpha}\PY{o}{/}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{theta} \PY{o}{=} \PY{n}{theta} \PY{o}{\PYZhy{}} \PY{n}{alpha\PYZus{}update} \PY{o}{*} \PY{n}{dt}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}
                 \PY{n}{theta\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{theta}\PY{p}{)}
                 \PY{n}{loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                 \PY{n}{loss\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{k}{return} \PY{n}{theta}\PY{p}{,} \PY{n}{theta\PYZus{}history}\PY{p}{,} \PY{n}{loss\PYZus{}history}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{t} \PY{o}{=} \PY{n}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}
         \PY{n}{t\PYZus{}est\PYZus{}decay}\PY{p}{,} \PY{n}{ts\PYZus{}decay}\PY{p}{,} \PY{n}{loss\PYZus{}decay} \PY{o}{=} \PY{n}{grad\PYZus{}desc\PYZus{}decay}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{t}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}
         
         \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{ts\PYZus{}decay}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{loss\PYZus{}decay}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{20} \PY{c+c1}{\PYZsh{} theta history and loss history are 20 items in them}
         \PY{k}{assert} \PY{n}{ts\PYZus{}decay}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{)} \PY{c+c1}{\PYZsh{} theta history contains theta values}
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{isscalar}\PY{p}{(}\PY{n}{loss}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} loss history should be a list of values, not vector}
         
         \PY{k}{assert} \PY{n}{loss\PYZus{}decay}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{loss\PYZus{}decay}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} loss is decreasing}
         
         \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{t\PYZus{}est\PYZus{}decay}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{4.5}\PY{p}{,} \PY{n}{atol}\PY{o}{=}\PY{l+m+mf}{2e\PYZhy{}1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} theta\PYZus{}est should be close to our value}
\end{Verbatim}


    \paragraph{Question 4c}\label{question-4c}

Let's visually inspect our results of running gradient descent to
optimize \(\boldsymbol\theta\). Plot our \(x\)-values with our model's
predicted \(y\)-values over the original scatter plot. Did gradient
descent successfully optimize \(\boldsymbol\theta\)?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{c+c1}{\PYZsh{} Run me}
         \PY{n}{t} \PY{o}{=} \PY{n}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}
         \PY{n}{t\PYZus{}est}\PY{p}{,} \PY{n}{ts}\PY{p}{,} \PY{n}{loss} \PY{o}{=} \PY{n}{grad\PYZus{}desc}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{t}\PY{p}{)}
         
         \PY{n}{t} \PY{o}{=} \PY{n}{init\PYZus{}t}\PY{p}{(}\PY{p}{)}
         \PY{n}{t\PYZus{}est\PYZus{}decay}\PY{p}{,} \PY{n}{ts\PYZus{}decay}\PY{p}{,} \PY{n}{loss\PYZus{}decay} \PY{o}{=} \PY{n}{grad\PYZus{}desc\PYZus{}decay}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{t}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{sin\PYZus{}model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{t\PYZus{}est}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{t\PYZus{}est}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Observation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gold}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_56_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    I think gradient descent successfully optimize \(\theta\) because the
model fits the observation quite well including the extra noise.

    \paragraph{Question 4d}\label{question-4d}

Let's compare our two gradient descent methods and see how they differ.
Plot the loss values over each iteration of gradient descent for both
static learning rate and decaying learning rate.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{} plt.plot(...) \PYZsh{} Plot of loss history for static learning rate}
         \PY{c+c1}{\PYZsh{} plt.plot(...) \PYZsh{} Plot of loss history for decaying learning rate}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{loss\PYZus{}decay}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_59_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Question 4e}\label{question-4e}

Compare and contrast the performance of the two gradient descent
methods. Which method begins to converge more quickly?

    These two gradient descent methods converge to loss levels which are
almost same. And the gradient descent method with decaying learning rate
converges more quickly to steady state.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{5: Visualizing Loss}\label{visualizing-loss}

\subsubsection{Question 5:}\label{question-5}

Let's visualize our loss functions and gain some insight as to how
gradient descent and stochastic gradient descent are optimizing our
model parameters.

\paragraph{Question 5a:}\label{question-5a}

In the previous plot is about the loss decrease over time, but what
exactly is path the theta value? Run the following three cells.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{c+c1}{\PYZsh{} Run me}
         \PY{n}{ts} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{ts}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
         \PY{n}{ts\PYZus{}decay} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{ts\PYZus{}decay}\PY{p}{)}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}
         \PY{n}{loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
         \PY{n}{loss\PYZus{}decay} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{loss\PYZus{}decay}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{c+c1}{\PYZsh{} Run me to see a 3D plot (gradient descent with static alpha)}
         \PY{n}{plot\PYZus{}3d}\PY{p}{(}\PY{n}{ts}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ts}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{loss}\PY{p}{,} \PY{n}{l2\PYZus{}loss}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    
    
    
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{c+c1}{\PYZsh{} Run me to see another 3D plot (gradient descent with decaying alpha)}
         \PY{n}{plot\PYZus{}3d}\PY{p}{(}\PY{n}{ts\PYZus{}decay}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ts\PYZus{}decay}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{loss\PYZus{}decay}\PY{p}{,} \PY{n}{l2\PYZus{}loss}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    
    
    
    
    In the following cell, write 1-2 sentences about the differences between
using a static learning rate and a learning rate with decay for gradient
descent. Use the loss history plot as well as the two 3D visualization
to support your answer.

    The speed of convergence of the gradient descent method with a decaying
learning rate is much higher than that of method with a static learning
rate. The loss history plot shows directly that the gradient descent
method with decaying learning rate converges more quickly to steady
state. The two 3D visualization show that gradient descent method with a
decaying learning rate converge in a zigzag path and gradient descent
method with a static alpha converge in a nearly linear path after the
first step. These lead to the inevitable fact that gradient descent
method with a decaying learning rate converges at a faster rate.

    \paragraph{Question 5b:}\label{question-5b}

Another common way of visualizing 3D dynamics is with a \emph{contour}
plot.

Please refer to this notebook when you are working on the next question:
Please refer to this notebook when you are working on the next question:
http://www.ds100.org/fa18/assets/lectures/lec09/09-Models-and-Estimation-II.html.
Search the page for \texttt{go.Contour}.

In next question, fill in the necessary part to create a contour plot.
Then run the following cells.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Run me}
         \PY{k+kn}{import} \PY{n+nn}{plotly}
         \PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{graph\PYZus{}objs} \PY{k}{as} \PY{n+nn}{go}
         \PY{n}{plotly}\PY{o}{.}\PY{n}{offline}\PY{o}{.}\PY{n}{init\PYZus{}notebook\PYZus{}mode}\PY{p}{(}\PY{n}{connected}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{k}{def} \PY{n+nf}{contour\PYZus{}plot}\PY{p}{(}\PY{n}{title}\PY{p}{,} \PY{n}{theta\PYZus{}history}\PY{p}{,} \PY{n}{loss\PYZus{}function}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    The function takes the following as argument:}
         \PY{l+s+sd}{        theta\PYZus{}history: a (N, 2) array of theta history}
         \PY{l+s+sd}{        loss: a list or array of loss value}
         \PY{l+s+sd}{        loss\PYZus{}function: for example, l2\PYZus{}loss}
         \PY{l+s+sd}{        model: for example, sin\PYZus{}model}
         \PY{l+s+sd}{        x: the original x input}
         \PY{l+s+sd}{        y: the original y output}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{theta\PYZus{}1\PYZus{}series} \PY{o}{=} \PY{n}{theta\PYZus{}history}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{c+c1}{\PYZsh{} a list or array of theta\PYZus{}1 value}
             \PY{n}{theta\PYZus{}2\PYZus{}series} \PY{o}{=} \PY{n}{theta\PYZus{}history}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{} a list or array of theta\PYZus{}2 value}
             
             \PY{c+c1}{\PYZsh{} Create trace of theta point}
             \PY{c+c1}{\PYZsh{} Uncomment the following lines and fill in the TODOS}
             \PY{n}{thata\PYZus{}points} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Scatter}\PY{p}{(}\PY{n}{name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Theta Values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                                       \PY{n}{x}\PY{o}{=}\PY{n}{theta\PYZus{}1\PYZus{}series}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO}
                                       \PY{n}{y}\PY{o}{=}\PY{n}{theta\PYZus{}2\PYZus{}series}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO}
                                       \PY{n}{mode}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lines+markers}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{}\PYZsh{} In the following block of code, we generate the z value}
             \PY{c+c1}{\PYZsh{}\PYZsh{} across a 2D grid}
             \PY{n}{t1\PYZus{}s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{theta\PYZus{}1\PYZus{}series}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{theta\PYZus{}1\PYZus{}series}\PY{p}{)} \PY{o}{+} \PY{l+m+mf}{0.1}\PY{p}{)}
             \PY{n}{t2\PYZus{}s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{theta\PYZus{}2\PYZus{}series}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{theta\PYZus{}2\PYZus{}series}\PY{p}{)} \PY{o}{+} \PY{l+m+mf}{0.1}\PY{p}{)}
         
             \PY{n}{x\PYZus{}s}\PY{p}{,} \PY{n}{y\PYZus{}s} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{t1\PYZus{}s}\PY{p}{,} \PY{n}{t2\PYZus{}s}\PY{p}{)}
             \PY{n}{data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{[}\PY{n}{x\PYZus{}s}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}s}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{T}
             \PY{n}{ls} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{t1}\PY{p}{,} \PY{n}{t2} \PY{o+ow}{in} \PY{n}{data}\PY{p}{:}
                 \PY{n}{l} \PY{o}{=} \PY{n}{loss\PYZus{}function}\PY{p}{(}\PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{t1}\PY{p}{,} \PY{n}{t2}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{)}
                 \PY{n}{ls}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{l}\PY{p}{)}
             \PY{n}{z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{ls}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Create the contour }
             \PY{c+c1}{\PYZsh{} Uncomment the following lines and fill in the TODOS}
             \PY{n}{lr\PYZus{}loss\PYZus{}contours} \PY{o}{=} \PY{n}{go}\PY{o}{.}\PY{n}{Contour}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{t1\PYZus{}s}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO }
                                           \PY{n}{y}\PY{o}{=}\PY{n}{t2\PYZus{}s}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO}
                                           \PY{n}{z}\PY{o}{=}\PY{n}{z}\PY{p}{,} \PY{c+c1}{\PYZsh{}TODO}
                                           \PY{n}{colorscale}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{reversescale}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{n}{plotly}\PY{o}{.}\PY{n}{offline}\PY{o}{.}\PY{n}{iplot}\PY{p}{(}\PY{n}{go}\PY{o}{.}\PY{n}{Figure}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{p}{[}\PY{n}{lr\PYZus{}loss\PYZus{}contours}\PY{p}{,} \PY{n}{thata\PYZus{}points}\PY{p}{]}\PY{p}{,} \PY{n}{layout}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{title}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{title}\PY{p}{\PYZcb{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{c+c1}{\PYZsh{} Run this}
         \PY{n}{contour\PYZus{}plot}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient Descent with Static Learning Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ts}\PY{p}{,} \PY{n}{l2\PYZus{}loss}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Run me}
         \PY{n}{contour\PYZus{}plot}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gradient Descent with Decay Learning Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ts\PYZus{}decay}\PY{p}{,} \PY{n}{l2\PYZus{}loss}\PY{p}{,} \PY{n}{sin\PYZus{}model}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}


    
    
    In the following cells, write down the answer to the following
questions: - How do you interpret the two contour plots? - Compare
contour plot and 3D plot, what are the pros and cons of each?

    Interpretation: The color of background denotes the values of loss. The
lighter color denotes the smaller loss corresponding a specific
\(\boldsymbol{\theta}\). And the orange line denotes the path of
changing of loss along with the improvement on \(\boldsymbol{\theta}\).
Besides, each marker point on the orange line denotes each pair of
\(\boldsymbol{\theta}\) with x-coordinate corresponding value of
\(\theta_1\) and y-coordinate corresponding value of \(\theta_2\).

Contour plot

Pros: - Easy to read the values of \(\theta_1\) and \(\theta_2\) by
reading the x-coordinate and y-coordinate of each marker point. - The
path of changing of loss is easy to interpret and imagine. - Color
denoting the values of loss is easy to distinguish for human's eye and
clear to understand.

Cons: - It is hard to read values of loss for each step. - The last few
optimization steps is hard to distinguish because of overplotting
resulting from concentrated marker points and lines.

3D plot

Pros: - Easy to examine the path of changing of loss in a
three-dimensional way because almost none overlapping exists. - Easy to
imagine the decreasing of the loss because it is denoted by height of
marker points in 3D plot. - Able to change perspective to get more
information of the changing path of loss.

Cons: - Hard to read the values of \(\theta_1\), \(\theta_2\) and loss
directly from the plot.

    \subsection{How to Improve?}\label{how-to-improve}

\subsubsection{Question 5c (optional)}\label{question-5c-optional}

Try adding the two additional model parameters for phase and amplitude
that we ignored (see 3a). What are the optimal phase and amplitude
values for your four parameter model? Do you get a better loss?

    \(f_\boldsymbol\theta(x) = \theta_1x + Asin(\theta_2x+\phi)\)

It should have a better loss because the model mentioned in 3a is a
special situation of the model above.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{k}{def} \PY{n+nf}{dt1\PYZus{}new}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute the numerical value of the partial of l2 loss with respect to theta\PYZus{}1}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of all x values}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of all y values}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{+}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{n}{x}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         
         \PY{k}{def} \PY{n+nf}{dt2\PYZus{}new}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute the numerical value of the partial of l2 loss with respect to theta\PYZus{}2}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of all x values}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of all y values}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{+}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{+}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{n}{x}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         
         \PY{k}{def} \PY{n+nf}{dt\PYZus{}A}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute the numerical value of the partial of l2 loss with respect to A}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of all x values}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of all y values}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{+}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{+}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         
         \PY{k}{def} \PY{n+nf}{dt\PYZus{}phi}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute the numerical value of the partial of l2 loss with respect to phi}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of all x values}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of all y values}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{+}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{+}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             
         \PY{k}{def} \PY{n+nf}{dt\PYZus{}up}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Returns the gradient of l2 loss with respect to vector theta}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{dt1\PYZus{}new}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{theta}\PY{p}{)}\PY{p}{,} \PY{n}{dt2\PYZus{}new}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{theta}\PY{p}{)}\PY{p}{,} \PY{n}{dt\PYZus{}A}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{,} \PY{n}{dt\PYZus{}phi}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{grad\PYZus{}desc\PYZus{}decay\PYZus{}new}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Run gradient descent update for a finite number of iterations and decaying learning rate}
         
         \PY{l+s+sd}{    Keyword arguments:}
         \PY{l+s+sd}{    x \PYZhy{}\PYZhy{} the vector of values x}
         \PY{l+s+sd}{    y \PYZhy{}\PYZhy{} the vector of values y}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the vector of values theta}
         \PY{l+s+sd}{    num\PYZus{}iter \PYZhy{}\PYZhy{} the max number of iterations}
         \PY{l+s+sd}{    alpha \PYZhy{}\PYZhy{} the learning rate}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Return:}
         \PY{l+s+sd}{    theta \PYZhy{}\PYZhy{} the optimal value of theta after num\PYZus{}iter of gradient descent}
         \PY{l+s+sd}{    theta\PYZus{}history \PYZhy{}\PYZhy{} the series of theta values over each iteration of gradient descent}
         \PY{l+s+sd}{    loss\PYZus{}history \PYZhy{}\PYZhy{} the series of loss values over each iteration of gradient descent}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{theta\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{loss\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}iter}\PY{p}{)}\PY{p}{:}
                 \PY{n}{alpha\PYZus{}update} \PY{o}{=} \PY{n}{alpha}\PY{o}{/}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{n}{theta} \PY{o}{=} \PY{n}{theta} \PY{o}{\PYZhy{}} \PY{n}{alpha\PYZus{}update} \PY{o}{*} \PY{n}{dt\PYZus{}up}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{theta}\PY{p}{)}
                 \PY{n}{theta\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{theta}\PY{p}{)}
                 \PY{n}{loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{\PYZhy{}}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x}\PY{o}{+}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                 \PY{n}{loss\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
             \PY{k}{return} \PY{n}{theta}\PY{p}{,} \PY{n}{theta\PYZus{}history}\PY{p}{,} \PY{n}{loss\PYZus{}history}
         
         \PY{n}{t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{t\PYZus{}est\PYZus{}decay\PYZus{}new}\PY{p}{,} \PY{n}{ts\PYZus{}decay\PYZus{}new}\PY{p}{,} \PY{n}{loss\PYZus{}decay\PYZus{}new} \PY{o}{=} \PY{n}{grad\PYZus{}desc\PYZus{}decay\PYZus{}new}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{t}\PY{p}{,} \PY{n}{num\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{loss\PYZus{}decay\PYZus{}new}\PY{p}{[}\PY{l+m+mi}{19}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{n}{loss\PYZus{}decay}\PY{p}{[}\PY{l+m+mi}{19}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:} -0.00064617207917094799
\end{Verbatim}
            
    So the result above shows that the loss of new model is a little better
than the two-parameter model. But it does not look like a obvious
improvement.

    \subsubsection{Question 5d (optional)}\label{question-5d-optional}

It looks like our basic two parameter model, a combination of a linear
function and sinusoidal function, was able to almost perfectly fit our
data. It turns out that many real world scenarios come from relatively
simple models.

At the same time, the real world can be incredibly complex and a simple
model wouldn't work so well. Consider the example below; it is neither
linear, nor sinusoidal, nor quadratic.

Optional: Suggest how we could iteratively create a model to fit this
data and how we might improve our results.

Extra optional: Try and build a model that fits this data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{x} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{10}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}\PY{p}{:}
             \PY{n}{r} \PY{o}{=} \PY{p}{(}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
             \PY{n}{x}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{r}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{)}
             \PY{n}{y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{r}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}  
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_81_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    I use polar transformation to model this data. We have \[r=t^2\]
\[x=rcos(t)\] \[y=rsin(t)\]

So, we can get the polar coordinates expression: \(r=\theta^2\)

where \[r=\sqrt{x^2+y^2}\] \[\theta = arctan(\frac{y}{x})\]

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{6: Short Analytic Problems}\label{short-analytic-problems}

Let's work through some problems to solidify the foundations of gradient
descent. If these questions are hard, consider reviewing lecture and
supplementary materials.

\subsubsection{Question 6}\label{question-6}

Complete the problems below. \textbf{Show your work and solution in
LaTeX}. Here are some useful examples of LaTex syntax:

Summation: \(\sum_{i=1}^n a_i\)

Exponent: \(a^2\)

Fraction: \(\frac{a}{b}\)

Multiplication: \(a \cdot b\)

Derivative: \(\frac{\partial}{\partial a}\)

Symbols: \(\alpha, \beta, \theta\)

    \subsubsection{Convexity}\label{convexity}

\paragraph{Question 6a}\label{question-6a}

In \href{http://www.ds100.org/fa18/syllabus\#lecture-week-5}{lecture 8},
we introduced the idea of a convex function. Let \(h(x) = f(x) + g(x)\)
where \(f,g\) are convex functions. Prove that \(h\) is convex.

    Proof:

Because f,g are convex function, we have:

\[tf(a)+(1-t)f(b)\geq f(ta+(1-t)b)\]
\[tg(a)+(1-t)g(b)\geq g(ta+(1-t)b)\] \[\forall a, \forall b, t\in[0,1]\]

Then, because h(x)=f(x)+g(x), we have:

\[
\begin{aligned}
th(a)+(1-t)h(b) &= t(f(a)+g(a))+(1-t)(f(b)+g(b))\\
& = [tf(a)+(1-t)f(a)]+[tg(a)+(1-t)g(a)]\\
& \geq f(ta+(1-t)b) +g(ta+(1-t)b)\\
& = h(ta+(1-t)b)
\end{aligned}
\]

So, via the definition of convexity, the h is convex.

    \subsubsection{Mutlivariable/vector calculus mechanical
problems}\label{mutlivariablevector-calculus-mechanical-problems}

\paragraph{Question 6b}\label{question-6b}

Show that the sum of the squared error
\[L(\textbf{w}) = ||\textbf{Xw}-\textbf{y}||_2^2\] can be expanded into
\[L(\textbf{w}) = \textbf{w}^T \textbf{X}^T \textbf{X w} - 2 \textbf{y}^T \textbf{X w} + \textbf{y}^T \textbf{y}\]
using vector/matrix notation.

    Proof:

\[
\begin{aligned}
L(\textbf{w}) &= ||\textbf{Xw}-\textbf{y}||_2^2\\
& =(\textbf{Xw}-\textbf{y})^T(\textbf{Xw}-\textbf{y})\\
& =((\textbf{Xw})^T-\textbf{y}^T)(\textbf{Xw}-\textbf{y})\\
& = \textbf{w}^T \textbf{X}^T \textbf{X w} - \textbf{y}^T \textbf{X w} -\textbf{w}^T \textbf{X}^T \textbf{y} + \textbf{y}^T \textbf{y}
\end{aligned}
\]

Because \(\textbf{y}^T \textbf{X w}\) and
\(\textbf{w}^T \textbf{X}^T \textbf{y}\) are both numbers and as we all
know, a number has the same value as its transpose.

So, we have:
\[L(\textbf{w}) = \textbf{w}^T \textbf{X}^T \textbf{X w} - 2 \textbf{y}^T \textbf{X w} + \textbf{y}^T \textbf{y}\]

    \paragraph{Question 6c}\label{question-6c}

Solve for the optimal \(\textbf{w}\), assuming \(\textbf{X}\) is full
rank. Use the Matrix Derivative rules from
\href{http://www.ds100.org/fa18/syllabus\#lecture-week-6}{lecture 11}.

    \[
\begin{aligned}
\frac{\partial L(\textbf{w})}{\partial \textbf{w}} & =2\textbf{X}^T \textbf{X}\textbf{w}-2\textbf{X}^T\textbf{y} 
\end{aligned}
\]

We need the solution for the optimal \(\textbf{w}\), so we let:
\[\frac{\partial L(\textbf{w})}{\partial \textbf{w}}=0\]

Then, we have:
\[\textbf{X}^T \textbf{X}\textbf{w}-\textbf{X}^T\textbf{y} =0\]
\[\textbf{X}^T \textbf{X}\textbf{w}=\textbf{X}^T\textbf{y} \]
\[\textbf{w}=(\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T\textbf{y} \]

This does not mean that
\(\textbf{w}=(\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T\textbf{y}\)
minimizes the \(L(\textbf{w})\). We are supposed to prove it.

Let
\(\hat{\textbf{w}}=(\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T\textbf{y}\)

for \(\forall \textbf{w}\), we have \[
\begin{aligned}
L(\textbf{w}) &= ||\textbf{Xw}-\textbf{y}||_2^2\\
&= ||\textbf{y}- \textbf{X}\hat{\textbf{w}}+\textbf{X}(\hat{\textbf{w}}-\textbf{w})||_2^2\\
&=||\textbf{y}- \textbf{X}\hat{\textbf{w}}||_2^2-(\hat{\textbf{w}}-\textbf{w})^T\textbf{X}^T\textbf{X}(\hat{\textbf{w}}-\textbf{w})+2(\hat{\textbf{w}}-\textbf{w})^T\textbf{X}^T(\textbf{y}-\textbf{X}\hat{\textbf{w}})
\end{aligned}
\]

Because
\(\textbf{X}^T \textbf{X}\hat{\textbf{w}}-\textbf{X}^T\textbf{y}=0\), so
we have \(\textbf{X}^T (\textbf{y}-\textbf{X}\hat{\textbf{w}})=0\)

\[L(\textbf{w}) = ||\textbf{y}- \textbf{X}\hat{\textbf{w}}||_2^2-(\hat{\textbf{w}}-\textbf{w})^T\textbf{X}^T\textbf{X}(\hat{\textbf{w}}-\textbf{w})\]

And \(\textbf{X}^T\textbf{X}\) is a non-negative definite matrix, so:
\[(\hat{\textbf{w}}-\textbf{w})^T\textbf{X}^T\textbf{X}(\hat{\textbf{w}}-\textbf{w})\geq0\]

So we have:

\[||\textbf{Xw}-\textbf{y}||_2^2\geq||\textbf{X}\hat{\textbf{w}}-\textbf{y}||_2^2\]

Finally, we reach the conclusion that:
\(\hat{\textbf{w}}=(\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T\textbf{y}\)
minimizes the \(L(\textbf{w})\)

    \paragraph{Question 6d}\label{question-6d}

Repeat the steps above for ridge regression as described in
\href{http://www.ds100.org/fa18/syllabus\#lecture-week-6}{lecture 12}.
Recall that ridge regression uses the following l2 regularized sum of
squared error.

\[L(\textbf{w}) = ||\textbf{X}\textbf{w} - \textbf{y}||_2^2 + \lambda ||\textbf{w}||_2^2\]

    \[
\begin{aligned}
L(\textbf{w}) &= ||\textbf{Xw}-\textbf{y}||_2^2 + \lambda ||\textbf{w}||_2^2\\
& =(\textbf{Xw}-\textbf{y})^T(\textbf{Xw}-\textbf{y})+\lambda \textbf{w}^T \textbf{w}\\
& =((\textbf{Xw})^T-\textbf{y}^T)(\textbf{Xw}-\textbf{y})+\lambda \textbf{w}^T \textbf{w}\\
& = \textbf{w}^T \textbf{X}^T \textbf{X w} - 2\textbf{y}^T \textbf{X w} + \textbf{y}^T \textbf{y}+\lambda \textbf{w}^T \textbf{w}
\end{aligned}
\]

The derivative of loss function is:
\[\frac{\partial L(\textbf{w})}{\partial \textbf{w}} =2\textbf{X}^T \textbf{X}\textbf{w}-2\textbf{X}^T\textbf{y}+2\lambda \textbf{I}\textbf{w} \]

Then, we have:
\[\textbf{X}^T \textbf{X}\textbf{w}-\textbf{X}^T\textbf{y} + 2\lambda \textbf{I}\textbf{w} =0\]
\[(\textbf{X}^T \textbf{X}+\lambda \textbf{I})\textbf{w}=\textbf{X}^T\textbf{y} \]
\[\textbf{w}=(\textbf{X}^T \textbf{X}+\lambda \textbf{I})^{-1}\textbf{X}^T\textbf{y} \]

We can use similar method as mentioned in 6c to prove that:

\(\textbf{w}=(\textbf{X}^T \textbf{X}+\lambda \textbf{I})^{-1}\textbf{X}^T\textbf{y}\)
minimizes the \(L(\textbf{w})\) of ridge regression.

    \paragraph{Question 6e}\label{question-6e}

Compare the analytic solutions of least squares and ridge regression.
Why does ridge regression guarantee that we can find a unique solution?
What are some of the tradeoffs (pros/cons) of using ridge regression?

    If \(\textbf{X}\) is not full rank, \(\textbf{X}^T \textbf{X}\) is not
invertible and there is no unique solution for \(\textbf{w}\).􏰄 This
problem does not occur with ridge regression, however. Because for any
design matrix \(\textbf{X}\), the quantity
\(\textbf{X}^T \textbf{X}+\lambda \textbf{I}\) is always invertible;
thus, there is always a unique solution for ridge regression.

Ridge regressions benefits from bias-variance trade-off. i.e. As
\(\lambda\) increases, the flexibility of ridge regression coefficients
decreases which means variance decreases but bias increases.

Pros - Ridge regression can reduce the variance with an increasing bias
and works best in situations where the OLS estimates have high variance.
- Can improve predictive performance. - Works in situations where
p\textless{}n. - Mathematically simple computations.

Cons - Ridge regression is not able to shrink coefficients to exactly
zero. - As a result, it can not perform variable selection.

    \subsubsection{Expectation and Variance}\label{expectation-and-variance}

\paragraph{Question 6f}\label{question-6f}

In \href{http://www.ds100.org/fa18/syllabus\#lecture-week-6}{lecture
10}, we completed half of the proof for the linearity of expectation.
Your task in this question is to complete the second half.

For reference, in lecture we showed that:

\[\mathbb{E}[aX + bY + c] = a\mathbb{E}[X] + \sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(x, y)by + c\]

To complete this proof, prove that:

\[b\mathbb{E}[Y] = \sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(x, y)by\]

Note: You cannot simply start with the given equation and use linearity
of expectation. Start with the summation on the right side and
manipulate it to get the left side.

Hint: What can we do with the order of the summations?

    \[
\begin{aligned}
\sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(x, y)by&=b\sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(x, y)y\\
&=b\sum_{y \in \mathbb{Y}}\sum_{x \in \mathbb{X}}P(x, y)y\\
&=b\sum_{y \in \mathbb{Y}}y\sum_{x \in \mathbb{X}}P(x, y)\\
Law \: of\: Total\: Probability\: &=b\sum_{y \in \mathbb{Y}}yP(Y=y)\\
&=b\mathbb{E}[Y]
\end{aligned}
\]

    \paragraph{Question 6g}\label{question-6g}

Prove that if two random variables \(X\) and \(Y\) are independent, then
\(Var(X - Y) = Var(X) + Var(Y)\).

    Proof:

\[
\begin{aligned}
Var(X-Y)&=\mathbb{E}[(X-Y)^2]-[\mathbb{E}(X-Y)]^2\\
&=\mathbb{E}[(X^2+Y^2-2XY)]-[\mathbb{E}(X)-\mathbb{E}(Y)]^2\\
&=\mathbb{E}(X^2)+\mathbb{E}(Y^2)-2\mathbb{E}(X)\mathbb{E}(Y)-[\mathbb{E}(X)]^2-[\mathbb{E}(Y))]^2+2\mathbb{E}(X)\mathbb{E}(Y)\\
&=Var(X)+Var(Y)
\end{aligned}
\]

To get the conclusion, we only need to prove that
\(\mathbb{E}(XY)=\mathbb{E}(X)\mathbb{E}(Y)\) when random variables
\(X\) and \(Y\) are independent. \[
\begin{aligned}
\mathbb{E}(XY)&=\sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(x, y)xy\\
Independence\:&=\sum_{x \in \mathbb{X}}\sum_{y \in \mathbb{Y}}P(X=x)P(Y=y)xy\\
&=\sum_{x \in \mathbb{X}}xP(X=x)\sum_{y \in \mathbb{Y}}P(Y=y)y\\
&=\mathbb{E}(X)\mathbb{E}(Y)
\end{aligned}
\] Finally, we reach the conclusion that
\(Var(X - Y) = Var(X) + Var(Y)\) when random variables \(X\) and \(Y\)
are independent.

    \subsection{7: Quick Regex Problems}\label{quick-regex-problems}

Here are some quick problems to review your knowledge of regular
expressions.

    \paragraph{Question 7a}\label{question-7a}

Write a regular expression to match the following strings without using
the \texttt{\textbar{}} operator.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Match:} \texttt{abcdefg}
\item
  \textbf{Match:} \texttt{abcde}
\item
  \textbf{Match:} \texttt{abc}
\item
  \textbf{Skip:} \texttt{c\ abc}
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{regxa} \PY{o}{=} \PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZca{}(abc).*}\PY{l+s+s2}{\PYZdq{}} \PY{c+c1}{\PYZsh{} fill in your pattern}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{k}{assert} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{|}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{regxa}\PY{p}{)}
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{search}\PY{p}{(}\PY{n}{regxa}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abc}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abc}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{search}\PY{p}{(}\PY{n}{regxa}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abcde}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abcde}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{search}\PY{p}{(}\PY{n}{regxa}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abcdefg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{abcdefg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{search}\PY{p}{(}\PY{n}{regxa}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c abc}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{)}
\end{Verbatim}


    \paragraph{Question 7b}\label{question-7b}

Write a regular expression to match the following strings without using
the \texttt{\textbar{}} operator.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Match:} \texttt{can}
\item
  \textbf{Match:} \texttt{man}
\item
  \textbf{Match:} \texttt{fan}
\item
  \textbf{Skip:} \texttt{dan}
\item
  \textbf{Skip:} \texttt{ran}
\item
  \textbf{Skip:} \texttt{pan}
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{regxb} \PY{o}{=} \PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{[cmf]}\PY{l+s+si}{\PYZob{}1\PYZcb{}}\PY{l+s+s2}{(an)}\PY{l+s+s2}{\PYZdq{}} \PY{c+c1}{\PYZsh{} fill in your pattern}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{k}{assert} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{|}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{regxb}\PY{p}{)}
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{can}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{can}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{fan}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{man}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{group}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ran}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{)} 
         \PY{k}{assert} \PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{match}\PY{p}{(}\PY{n}{regxb}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{)}
\end{Verbatim}


    \paragraph{Question 7c:}\label{question-7c}

Write a regular expression to extract and print the quantity and type of
objects in a string. You may assume that a space separates quantity and
type, ie. \texttt{"\{quantity\}\ \{type\}"}. See the example string
below for more detail.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Hint:} use \texttt{re.findall}
\item
  \textbf{Hint:} use \texttt{\textbackslash{}d} for digits and one of
  either \texttt{*} or \texttt{+}.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{text\PYZus{}qc} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{I}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ve got 10 eggs that I stole from 20 gooses belonging to 30 giants.}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{n}{res\PYZus{}qc} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{findall}\PY{p}{(}\PY{n}{pattern}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{d+}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{s[a\PYZhy{}z]*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{string}\PY{o}{=}\PY{n}{text\PYZus{}qc}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         
         \PY{n}{res\PYZus{}qc}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}45}]:} ['10 eggs', '20 gooses', '30 giants']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{k}{assert} \PY{n}{res\PYZus{}qc} \PY{o}{==} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{10 eggs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{20 gooses}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{30 giants}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \paragraph{Question 7d:}\label{question-7d}

Write a regular expression to replace at most 2 occurrences of space,
comma, or dot with a colon.

\textbf{Hint:} use
\texttt{re.sub(regex,\ "newtext",\ string,\ number\_of\_occurences)}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{n}{text\PYZus{}qd} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Python Exercises, PHP exercises.}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{res\PYZus{}qd} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{n}{pattern}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{s,.]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{string}\PY{o}{=}\PY{n}{text\PYZus{}qd}\PY{p}{,}\PY{n}{repl}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{count}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)} \PY{c+c1}{\PYZsh{} Hint: use re.sub()}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         
         \PY{n}{res\PYZus{}qd}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}47}]:} 'Python:Exercises: PHP exercises.'
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{k}{assert} \PY{n}{res\PYZus{}qd} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Python:Exercises: PHP exercises.}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}


    \paragraph{Question 7e (optional):}\label{question-7e-optional}

Write a regular expression to replace all words that are not
\texttt{"mushroom"} with \texttt{"badger"}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n}{text\PYZus{}qe} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{this is a word mushroom mushroom}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{res\PYZus{}qe} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{n}{pattern}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{b(?!mushroom)}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{b}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{string}\PY{o}{=}\PY{n}{text\PYZus{}qe}\PY{p}{,}\PY{n}{repl}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{badger}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} Hint: https://www.regextester.com/94017}
         \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
         \PY{n}{res\PYZus{}qe}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}49}]:} 'badger badger badger badger mushroom mushroom'
\end{Verbatim}
            
    \subsection{Submission - IMPORTANT, PLEASE
READ}\label{submission---important-please-read}

For this assignment and future assignments (homework and projects) you
will also submit your free response and plotting questions to
gradescope. To do this, you can download as PDF
(\texttt{File-\textgreater{}Download\ As-\textgreater{}PDF\ via\ Latex\ (.pdf)}).
You are responsible for submitting and tagging your answers in
gradescope. For each free response and plotting question, please
include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Relevant code used to generate the plot or inform your insights
\item
  The written free response or plot
\end{enumerate}

We are doing this to make it easier on our graders and for you, in the
case you need to submit a regrade request. Gradescope (as of now) is
still better for manual grading.

    \subsection{Submission}\label{submission}

You're done!

Before submitting this assignment, ensure to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Restart the Kernel (in the menubar, select
  Kernel-\textgreater{}Restart \& Run All)
\item
  Validate the notebook by clicking the "Validate" button
\end{enumerate}

Finally, make sure to \textbf{submit} the assignment via the Assignments
tab in Datahub


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
